\section{Zur Moral fähige KI}\label{sec::moralische KI}

\subsection{Artificial Morality (AM)}\label{subsec::artificial morality}

Als Artificial Morality, wird die Fähigkeit einer künstlichen Maschine oder Intelligenz, moralische Entscheidungen zu
treffen, bezeichnet.\footnote{vgl. Misselhorn, Catrin. Maschinenethik und "Artificial Morality"} Spezifisch
wird dabei erwartet, dass die KI verschiedene Variablen, die einen moralischen Wert haben, in den Entscheidungsprozess
einbringen kann und anschließend, aufgrund der bestehenden Moralvorstellungen, diese Entscheidung ausführt.\footnote{vlg. Misselhorn C. Artificial Moral Agents: Conceptual Issues and Ethical Controversy}

\subsubsection{Information zu weiteren Details}

Die AM ist ein sehr stark diskutiertes Thema, denn es hat die Kapazitäten unsere technologische Welt wie wir sie kennen
Grundlegend zu verändern.
Hier reichen die Meinungen von vollkommener Ablehnung der AM bis zur kompletten Befürwortung.
Die ethischen Grundsätze, Regelungen und Diskussionen in Verbindung mit der künstlichen Moral ist ein großes Thema
in sich, welches hier nicht weiter im Detail betrachtet wird (Das Buch "Responsible Artificial Intelligence"
\footnote{Misselhorn C. Artificial Moral Agents: Conceptual Issues and Ethical Controversy} geht hier tiefer ins Detail)

\subsection{Anwendungsmöglichkeiten einer moralischen KI}\label{subsec::anwendungen}

Es gibt viele verschiedene alltägliche und wissenschaftliche Anwendungszwecke für eine mit Artificial Morality ausgestattete KI\@.

\subsubsection{Autonomes Fahren}\label{subsubsec::autonomes fahren}

Eine mögliche Anwendung für die Artificial Morality wird beim autonomen Fahren diskutiert.
In Gefahrensituationen wird sich das Auto, auf einer vorgelegten Moralvorstellung, mit allen vorliegenden und erfassten
Daten entscheiden müssen, wie es in einer Situation zu handeln hat.
Ein Grundsatz bei einer solchen Maschine wird wahrscheinlich sein, dass das menschliche Leben immer Vorrang hat, bei
Gefahrensituationen.
Aber auch Sach- und Tierschäden sind zu vermeiden, deshalb wird sich die KI entscheiden müssen, wann sie versuchte das
Tierleben zu retten und wann es zu gefährlich für den Menschen im Auto ist.
Nach solchen Problemen wird sich eine KI im Auto mit verschiedenen ethischen Dilemmas auseinandersetzen müssen.
Zum Beispiel wird die KI mit einer Abwandlung des Trolley-Problems (\hyperlink{Trolley-Problem}{siehe Trolley-Problem, Utilitarismus}) konfrontiert werden und muss ihrer moralischen Ausrichtung
folgend eine möglicherweise folgenschwere Entscheidung treffen.

\subsubsection{Pflege}\label{subsubsec::pflege}

Auch in der Krankenpflege erhofft man sich große Hilfe von AMs. Mit dem steigenden demografischen Wandel werden rasant mehr
Menschen in einer pflegebedürftigen Lage sein.\footnote{vgl. Misselhorn, Catrin. Maschinenethik und "Artificial Morality"}
Die moralischen KIs können in diesem Fall entscheiden, wann und wie oft ein Patient an die Zunahme von Essen und Medikamenten
erinnert werden soll.
Im viel gravierenderen Beispiel wird sich die KI auch entscheiden müssen, ob und wann sie den Krankenwagen ruft, wenn sich ein
Patient eine Zeitlang nicht mehr bewegt hat.

In dem Fallbeispiel der Pflege prallen jedoch viele moralische Fragen aufeinander, die nicht nur das physische Wohlwollen des Patienten,
sondern auch die Privatsphäre, Selbstständigkeit und psychische Gesundheit des Patienten betreffen.\footnote{vlg. Misselhorn C. Artificial Moral Agents: Conceptual Issues and Ethical Controversy}

\subsubsection{Saugroboter}\label{subsubsec::saugroboter}

Nicht nur in großen Projekten und Institutionen, wie der Pflege (~\ref{subsubsec::pflege}) und beim autonomen Fahren (~\ref{subsubsec::autonomes fahren})
kann eine moralische KI eingesetzt werden.
Ein Saugroboter kann ebenfalls mit moralischen Entscheidungen konfrontiert werden.
Sei es nun eine Entscheidung, ob es den Marienkäfer einsaugen, umfahren oder sogar wegjagen sollte.
Oder was macht es mit einer Spinne?
Soll der Roboter der Spinne folgen und sie mit Absicht einsaugen, da viele Menschen Angst vor Spinnen haben oder auch
die Spinne verschonen?

Moralische Entscheidungen lassen sich schon auf den unkompliziertesten Ebenen von künstlichen Systemen und Maschinen beobachten
und sie geben damit einen guten Anlass, für die Wissenschaft, die Artificial Morality weiterzuentwickeln.

